# -*- coding: utf-8 -*-
"""etl_script.py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hrbgW9EsQjycphHoAHmeYWvbLSf53laA
"""

import pandas as pd
import os
from datetime import datetime
from supabase import create_client, Client
from sentiment_analysis_using_indobert import analyze_sentiment

def scrape_twitter_data(search_keywords, filename, limit, twitter_auth_token):
    for keyword in search_keywords:
        command = f'npx -y tweet-harvest@2.6.1 -o "{filename}" -s "{keyword}" --tab "LATEST" -l {limit} --token "{twitter_auth_token}"'
        print(f"Running command: {command}")
        os.system(command)
        remove_duplicates_and_add_column(filename, keyword)

def remove_duplicates_and_add_column(filename, keyword):
    file_path = f"data/{filename}"
    df = pd.read_csv(file_path, delimiter=",")
    print(f"Columns in scraped data: {df.columns.tolist()}")
    if 'full_text' not in df.columns:
        raise KeyError("The column 'full_text' was not found in the scraped data.")
    df['jenis_program'] = keyword
    df.drop_duplicates(inplace=True)
    preprocess_and_save(df, filename, keyword)

def preprocess_and_save(df, filename, keyword):
    # Sentiment analysis without cleaning
    df['sentiment'] = df['full_text'].apply(analyze_sentiment)
    # Drop unnecessary columns
    df = df[['full_text', 'sentiment', 'jenis_program']]
    # Save to CSV
    output_filename = f'data/scrapped_{filename.split(".")[0]}_{keyword.replace(" ", "_")}.csv'
    df.to_csv(output_filename, index=False)
    # Upsert to Supabase
    upsert_to_supabase(df)

def upsert_to_supabase(df):
    url: str = os.getenv("SUPABASE_URL")
    key: str = os.getenv("SUPABASE_KEY")
    supabase: Client = create_client(url, key)
    data = df.to_dict(orient='records')
    for record in data:
        response = supabase.table("tweets").upsert(record).execute()
        if response.error:
            print(f"Error upserting record: {response.error}")

# Get the first day of the current month and today's date in the format YYYY-MM-DD
first_day_of_month = datetime.today().replace(day=1).strftime('%Y-%m-%d')
today = datetime.today().strftime('%Y-%m-%d')

# Keyword list with dynamic dates
search_keywords = [
    f'PKH lang:id since:{first_day_of_month} until:{today}',
    f'Program Indonesia Pintar lang:id since:{first_day_of_month} until:{today}',
    f'Program Indonesia Sehat lang:id since:{first_day_of_month} until:{today}',
    f'Sembako lang:id since:{first_day_of_month} until:{today}',
    f'Bansos lang:id since:{first_day_of_month} until:{today}',
    f'BPNT lang:id since:{first_day_of_month} until:{today}',
    f'BLT lang:id since:{first_day_of_month} until:{today}',
    f'Komunitas Adat Terpencil lang:id since:{first_day_of_month} until:{today}',
    f'Beras Sejahtera lang:id since:{first_day_of_month} until:{today}',
    f'Raskin lang:id since:{first_day_of_month} until:{today}',
    f'Kelompok Usaha Bersama lang:id since:{first_day_of_month} until:{today}',
    f'PKK lang:id since:{first_day_of_month} until:{today}'
]

filename = 'kemensos.csv'
limit = 500
twitter_auth_token = os.getenv('TWITTER_AUTH_TOKEN')

# Scrape Twitter data for each keyword
scrape_twitter_data(search_keywords, filename, limit, twitter_auth_token)
